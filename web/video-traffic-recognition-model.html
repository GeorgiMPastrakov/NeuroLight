<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Car Counter ‚Äî Upload Video (Works)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { color-scheme: dark; }
    body { margin:0; background:#0b0b0c; color:#eaeaea; font:15px/1.4 system-ui,sans-serif }
    .wrap { max-width: 980px; margin: auto; padding: 18px; display:grid; gap:14px }
    .row { display:flex; gap:12px; flex-wrap:wrap; align-items:center }
    .btn { padding:10px 14px; border-radius:10px; background:#252530; border:1px solid #3a3a46; cursor:pointer }
    .badge { padding:6px 10px; border-radius:999px; background:#1e1e22 }
    input[type="file"] { display:none }
    .stage { position:relative; width:100%; aspect-ratio:16/9; background:#0f1115; border:1px solid #242434; border-radius:12px; overflow:hidden }
    video, canvas { position:absolute; inset:0; width:100%; height:100%; object-fit:contain }
    #log { font-size:12px; opacity:.85 }
  </style>
</head>
<body>
  <div class="wrap">
    <h1>Car Counter (upload a video)</h1>

    <div class="row">
      <label class="btn" for="file">üéûÔ∏è Choose video</label>
      <input id="file" type="file" accept="video/*" />
      <span id="status" class="badge">Model: loading‚Ä¶</span>
      <span id="count" class="badge">Cars: ‚Äî</span>
    </div>

    <div class="stage">
      <video id="vid" controls playsinline muted></video>
      <canvas id="overlay"></canvas>
    </div>

    <div id="log" class="badge">log: ‚Äî</div>
  </div>

  <script type="module">
    import { pipeline, env } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0";

    // ‚îÄ‚îÄ‚îÄ ONLY KNOBS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const CONFIDENCE = 0.75;   // certainty threshold (try 0.80 if your clip is noisy)
    const NMS_IOU    = 0.50;   // fixed, class-aware de-dup
    // ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    env.allowLocalModels = false; // download model from the Hub, cached by browser
    const VEHICLES = new Set(["car","truck","bus","motorcycle"]);

    const statusEl = document.getElementById("status");
    const countEl  = document.getElementById("count");
    const fileEl   = document.getElementById("file");
    const video    = document.getElementById("vid");
    const overlay  = document.getElementById("overlay");
    const ctx      = overlay.getContext("2d");
    const logEl    = document.getElementById("log");
    const log = (...a) => { console.log("[counter]", ...a); logEl.textContent = "log: " + a.join(" "); };

    // overlay sizing: match CSS size, draw in CSS pixels using DPR scaling
    function resizeOverlay() {
      const r = overlay.getBoundingClientRect();
      const dpr = window.devicePixelRatio || 1;
      overlay.width  = Math.max(1, Math.round(r.width  * dpr));
      overlay.height = Math.max(1, Math.round(r.height * dpr));
      ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
      log("overlay resized", r.width + "x" + r.height);
    }
    window.addEventListener("resize", resizeOverlay);

    // map normalized boxes onto letterboxed video (‚Äúobject-fit: contain‚Äù)
    function getContainRect(mediaW, mediaH, viewW, viewH) {
      const scale = Math.min(viewW / mediaW, viewH / mediaH);
      const w = mediaW * scale, h = mediaH * scale;
      const x = (viewW - w) / 2, y = (viewH - h) / 2;
      return { x, y, w, h };
    }

    // IoU + simple, class-aware NMS
    function iou(a, b) {
      const ax2 = a.x + a.w, ay2 = a.y + a.h;
      const bx2 = b.x + b.w, by2 = b.y + b.h;
      const ix = Math.max(0, Math.min(ax2, bx2) - Math.max(a.x, b.x));
      const iy = Math.max(0, Math.min(ay2, by2) - Math.max(a.y, b.y));
      const inter = ix * iy;
      const ua = a.w * a.h + b.w * b.h - inter;
      return ua > 0 ? inter / ua : 0;
    }
    function nms(dets) {
      const byClass = {};
      for (const d of dets) (byClass[d.label] ||= []).push(d);
      const out = [];
      for (const cls in byClass) {
        const arr = byClass[cls].sort((a,b)=>b.score-a.score);
        while (arr.length) {
          const keep = arr.shift();
          out.push(keep);
          for (let i = arr.length - 1; i >= 0; i--) {
            if (iou(keep.box, arr[i].box) > NMS_IOU) arr.splice(i,1);
          }
        }
      }
      return out;
    }

    // load DETR (COCO) ‚Äî runs in-tab (WebGPU if available, else WASM)
    let detector = null;
    async function loadModel() {
      statusEl.textContent = "Loading model‚Ä¶";
      log("loading DETR (COCO) via Transformers.js ‚Ä¶");
      detector = await pipeline("object-detection", "Xenova/detr-resnet-50", { device: "webgpu" });
      statusEl.textContent = "Ready ‚úÖ";
      log("model ready");
    }
    await loadModel();

    // detection work happens from an offscreen canvas (ensures stable input)
    const work = document.createElement("canvas");
    const wctx = work.getContext("2d");

    function drawBoxes(dets) {
      const r = overlay.getBoundingClientRect();
      ctx.clearRect(0, 0, r.width, r.height);

      const vw = video.videoWidth, vh = video.videoHeight;
      if (!vw || !vh) return;
      const rect = getContainRect(vw, vh, r.width, r.height);

      ctx.strokeStyle = "rgba(155,201,255,.95)";
      ctx.lineWidth = 2;
      ctx.font = "12px system-ui, sans-serif";

      for (const d of dets) {
        const x = rect.x + d.box.x * rect.w;
        const y = rect.y + d.box.y * rect.h;
        const w = d.box.w * rect.w;
        const h = d.box.h * rect.h;

        ctx.strokeRect(x, y, w, h);
        const tag = `${d.label} ${(d.score*100|0)}%`;
        const tw = ctx.measureText(tag).width + 8;
        ctx.fillStyle = "rgba(155,201,255,.95)";
        ctx.fillRect(x-2, y-18, tw, 16);
        ctx.fillStyle = "#0b0b0c";
        ctx.fillText(tag, x+2, y-5);
      }
    }

    // Frame-driven loop using requestVideoFrameCallback (rvfc)
    let rvfcId = null;
    let lastDetectTs = 0;
    const MIN_INTERVAL_MS = 800; // ~1 fps; change if you want faster/slower

    async function onFrame(now, metadata) {
      try {
        // throttle detections
        if (!video.paused && !video.ended && (now - lastDetectTs) >= MIN_INTERVAL_MS) {
          lastDetectTs = now;

          // prepare work canvas ~640px wide (keeps model input reasonable)
          const vw = video.videoWidth || 640, vh = video.videoHeight || 360;
          const targetW = 640;
          const targetH = Math.max(1, Math.round(targetW * (vh / vw)));
          work.width = targetW; work.height = targetH;
          wctx.drawImage(video, 0, 0, work.width, work.height);

          statusEl.textContent = "Detecting‚Ä¶";
          log("detect @t=", (metadata?.mediaTime ?? 0).toFixed(2), "s");

          const raw = await detector(work, { threshold: CONFIDENCE, percentage: true });
          let dets = raw.map(r => ({
            label: r.label, score: r.score,
            box: { x: r.box.xmin, y: r.box.ymin, w: r.box.xmax - r.box.xmin, h: r.box.ymax - r.box.ymin }
          })).filter(r => VEHICLES.has(r.label));

          dets = nms(dets);
          drawBoxes(dets);
          countEl.textContent = `Cars: ${dets.length}`;
          statusEl.textContent = "Ready ‚úÖ";
        }
      } catch (e) {
        console.error(e);
        statusEl.textContent = "Error üòµ";
        log("error:", e.message || e);
      } finally {
        rvfcId = video.requestVideoFrameCallback(onFrame);
      }
    }

    function startRvfc() {
      if (rvfcId) video.cancelVideoFrameCallback(rvfcId);
      resizeOverlay();
      rvfcId = video.requestVideoFrameCallback(onFrame);
      log("rvfc started");
    }
    function stopRvfc() {
      if (rvfcId) { video.cancelVideoFrameCallback(rvfcId); rvfcId = null; log("rvfc stopped"); }
    }

    // choose a local video
    fileEl.addEventListener("change", e => {
      const file = e.target.files?.[0];
      if (!file) return;
      stopRvfc();
      const url = URL.createObjectURL(file);
      video.src = url;
      video.onloadedmetadata = () => {
        log("video loaded", video.videoWidth + "x" + video.videoHeight);
        video.play();
      };
    });

    // hook up lifecycle
    video.addEventListener("loadedmetadata", resizeOverlay);
    video.addEventListener("play", startRvfc);
    video.addEventListener("pause", stopRvfc);
    video.addEventListener("ended", stopRvfc);

    // note: run via a local server so the model can be fetched/cached.
  </script>
</body>
</html>
